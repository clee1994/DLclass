{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW03_backprop.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/clee1994/DLclass/blob/master/HW03_backprop.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "FJiUPMWbdDLP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "f0546207-a457-42d1-89a4-7556746276ce"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x, Deriv=False):\n",
        "  if Deriv:\n",
        "    out = sigmoid(x)\n",
        "    return out*(1-out)\n",
        "  else:\n",
        "    return 1/(1+np.exp(-x))\n",
        "  \n",
        "  \n",
        "class NN:\n",
        "  \n",
        "  # define some NN params\n",
        "  numLayers = 2\n",
        "  shape = None\n",
        "  weights = []\n",
        "  \n",
        "  \n",
        "  \n",
        "  # Initialize the network based on the number of neurons per layer\n",
        "  def __init__(self,neuronsPerLayer):\n",
        "    \"\"\"Initialization of Neural Network\"\"\"\n",
        "  \n",
        "  # initialize some parameters\n",
        "    self.numLayers = len(neuronsPerLayer) - 1\n",
        "    self.shape = neuronsPerLayer\n",
        "    \n",
        "    # null the network\n",
        "    self._layerInput=[]\n",
        "    self._layerOutput=[]\n",
        "    \n",
        "    #initialize the weights\n",
        "    for (l1,l2) in zip (neuronsPerLayer[:-1],neuronsPerLayer[1:]):\n",
        "      self.weights.append(np.random.normal(scale=.1,size=(l2,l1+1))) # List of weights\n",
        "  \n",
        "  #Create the forward pass\n",
        "  def forward(self,input):\n",
        "    \"\"\"Define the forward pass for this network\"\"\"\n",
        "    numExamples = input.shape[0]\n",
        "    \n",
        "    self._layerInput =[]\n",
        "    self._layerOutput =[]\n",
        "    \n",
        "    for idx in range(self.numLayers):\n",
        "      if idx == 0:\n",
        "        intermediate = np.vstack([input.T, np.ones([1,numExamples])])\n",
        "      else:\n",
        "        intermediate = np.vstack([self._layerOutput[-1],np.ones([1,numExamples])])        \n",
        "      # for each layer \n",
        "      \n",
        "      #value before activation is the weight times the previous input + bias\n",
        "      layerInput = self.weights[idx].dot(intermediate)\n",
        "      \n",
        "      #Store all inputs and outputs\n",
        "      self._layerInput.append(layerInput)\n",
        "      self._layerOutput.append(sigmoid(layerInput))\n",
        "\n",
        "    return self._layerOutput[-1].T\n",
        "    \n",
        "  def backward(self,input,target,lrate = .1):\n",
        "    \"\"\"Define the backward pass for this network\"\"\"\n",
        "    Err = []\n",
        "    numExamples = input.shape[0]\n",
        "    \n",
        "    #In order to update the weights, first implement the forward pass    \n",
        "    self.forward(input)\n",
        "    \n",
        "    # now work backwards through the layers\n",
        "    for rev_idx in reversed(range(self.numLayers)):      \n",
        "      # For every layer find the error at that layer backwards\n",
        "      \n",
        "      # First calculate the gradient due to the sigmoid at this layer\n",
        "      sigmoidGradient = sigmoid(self._layerInput[rev_idx],True)\n",
        "      \n",
        "      if rev_idx == self.numLayers -1: # if last layer find loss vs. Target        \n",
        "        output_Err = self._layerOutput[rev_idx]-target.T\n",
        "        Loss = np.sum(output_Err**2)\n",
        "\n",
        "      else:\n",
        "        pass\n",
        "      \n",
        "    return Loss\n",
        "    \n",
        "Input = np.array([[0,0,0,0],[1,1,1,1],[0,1,1,1],[1,0,1,1]])\n",
        "Target = np.array([[0.0],[0.0],[1.0],[1.0]])\n",
        "\n",
        "\n",
        "# Neurons per layer\n",
        "# Input = 4\n",
        "# Hidden1 = 4\n",
        "# Hidden2 = 3\n",
        "# Output = Target = 1\n",
        "bpn = NN((4,4,3,1))\n",
        "\n",
        "# print(\"The initial weights are:\",NN.weights)\n",
        "\n",
        "Error = bpn.backward(Input, Target)\n",
        "Output = bpn.forward(Input)\n",
        "\n",
        "print ('Input \\t\\tOutput \\t\\tTarget')\n",
        "for i in range(Input.shape[0]):\n",
        "    print ('{0}\\t {1} \\t{2}'.format(Input[i], Output[i], Target[i]))\n",
        "      \n",
        "  \n",
        "\n",
        "maxIterations = 100000\n",
        "minError = .001\n",
        "print ('\\n')\n",
        "for i in range(maxIterations + 1):\n",
        "    Error = bpn.backward(Input, Target)\n",
        "    if i % 2500 == 0:\n",
        "        print(\"Iteration {0}\\tError: {1:0.6f}\".format(i,Error))\n",
        "    if Error <= minError:\n",
        "        print(\"Minimum error reached at iteration {0}\".format(i))\n",
        "        break\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input \t\tOutput \t\tTarget\n",
            "[0 0 0 0]\t [0.51788474] \t[0.]\n",
            "[1 1 1 1]\t [0.51794725] \t[0.]\n",
            "[0 1 1 1]\t [0.51784351] \t[1.]\n",
            "[1 0 1 1]\t [0.51799827] \t[1.]\n",
            "\n",
            "\n",
            "Iteration 0\tError: 1.001275\n",
            "Iteration 2500\tError: 1.001275\n",
            "Iteration 5000\tError: 1.001275\n",
            "Iteration 7500\tError: 1.001275\n",
            "Iteration 10000\tError: 1.001275\n",
            "Iteration 12500\tError: 1.001275\n",
            "Iteration 15000\tError: 1.001275\n",
            "Iteration 17500\tError: 1.001275\n",
            "Iteration 20000\tError: 1.001275\n",
            "Iteration 22500\tError: 1.001275\n",
            "Iteration 25000\tError: 1.001275\n",
            "Iteration 27500\tError: 1.001275\n",
            "Iteration 30000\tError: 1.001275\n",
            "Iteration 32500\tError: 1.001275\n",
            "Iteration 35000\tError: 1.001275\n",
            "Iteration 37500\tError: 1.001275\n",
            "Iteration 40000\tError: 1.001275\n",
            "Iteration 42500\tError: 1.001275\n",
            "Iteration 45000\tError: 1.001275\n",
            "Iteration 47500\tError: 1.001275\n",
            "Iteration 50000\tError: 1.001275\n",
            "Iteration 52500\tError: 1.001275\n",
            "Iteration 55000\tError: 1.001275\n",
            "Iteration 57500\tError: 1.001275\n",
            "Iteration 60000\tError: 1.001275\n",
            "Iteration 62500\tError: 1.001275\n",
            "Iteration 65000\tError: 1.001275\n",
            "Iteration 67500\tError: 1.001275\n",
            "Iteration 70000\tError: 1.001275\n",
            "Iteration 72500\tError: 1.001275\n",
            "Iteration 75000\tError: 1.001275\n",
            "Iteration 77500\tError: 1.001275\n",
            "Iteration 80000\tError: 1.001275\n",
            "Iteration 82500\tError: 1.001275\n",
            "Iteration 85000\tError: 1.001275\n",
            "Iteration 87500\tError: 1.001275\n",
            "Iteration 90000\tError: 1.001275\n",
            "Iteration 92500\tError: 1.001275\n",
            "Iteration 95000\tError: 1.001275\n",
            "Iteration 97500\tError: 1.001275\n",
            "Iteration 100000\tError: 1.001275\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}